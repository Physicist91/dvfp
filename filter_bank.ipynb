{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Deep Vision\" architecture\n",
    "\n",
    "![title](arch-simplified.png)\n",
    "\n",
    "<img src=\"side-branch.png\" alt=\"drawing\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# begin by importing our \"Deep Vision\" module (or dv in short)\n",
    "import dv\n",
    "from dv.model import DeepVision_VGG16\n",
    "from dv.ImageFolder import CarsDataset\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image, ImageOps\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paper use input size of 448 x 448, we will use random crop to this size\n",
    "\n",
    "def scale_keep_ar_min_fixed(img, fixed_min):\n",
    "    ow, oh = img.size\n",
    "    if ow < oh:      \n",
    "        nw = fixed_min\n",
    "        nh = nw * oh // ow   \n",
    "    else:      \n",
    "        nh = fixed_min \n",
    "        nw = nh * ow // oh\n",
    "    return img.resize((nw, nh), Image.BICUBIC)\n",
    "\n",
    "def transform_train():\n",
    "    transform_list = []\n",
    "    transform_list.append(torchvision.transforms.Lambda(lambda x:scale_keep_ar_min_fixed(x, 448)))\n",
    "    transform_list.append(torchvision.transforms.RandomHorizontalFlip(p=0.3))\n",
    "    transform_list.append(torchvision.transforms.RandomCrop((448, 448)))\n",
    "    transform_list.append(torchvision.transforms.ToTensor())\n",
    "    transform_list.append(torchvision.transforms.Normalize(mean=(0.5,0.5,0.5),std=(0.5,0.5,0.5)))\n",
    "    return transforms.Compose(transform_list)\n",
    "\n",
    "def transform_test():\n",
    "    transform_list = []\n",
    "    transform_list.append(transforms.Lambda(lambda x:scale_keep_ar_min_fixed(x, 560)))\n",
    "    transform_list.append(transforms.TenCrop(448)) \n",
    "    transform_list.append(transforms.Lambda(lambda crops: torch.stack([transforms.Normalize(mean=(0.5,0.5,0.5),std=(0.5,0.5,0.5))((transforms.ToTensor())(crop)) for crop in crops])) )\n",
    "    return transforms.Compose(transform_list)\n",
    "\n",
    "def transform_test_simple():\n",
    "    transform_list = []\n",
    "    transform_list.append(transforms.Lambda(lambda x:scale_keep_ar_min_fixed(x, 448)))\n",
    "    transform_list.append(transforms.CenterCrop((448, 448)))\n",
    "    transform_list.append(transforms.ToTensor())\n",
    "    transform_list.append(transforms.Normalize(mean=(0.5,0.5,0.5),std=(0.5,0.5,0.5)))\n",
    "    return transforms.Compose(transform_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "momentum = 0.9\n",
    "weight_decay = 0.000005\n",
    "net = DeepVision_VGG16(k = 10, M = 200)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=momentum, weight_decay = weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = '/Users/kevinsiswandi/dvfp/data'\n",
    "train_dataset = CarsDataset(os.path.join(img_dir,'devkit/cars_train_annos.mat'),\n",
    "                            os.path.join(img_dir,'cars_train'),\n",
    "                            os.path.join(img_dir,'devkit/cars_meta.mat'),\n",
    "                            cleaned=None,\n",
    "                            transform=transform_train\n",
    "                            )\n",
    "\n",
    "test_dataset = CarsDataset(os.path.join(img_dir,'devkit/cars_test_annos_withlabels.mat'),\n",
    "                            os.path.join(img_dir,'cars_test'),\n",
    "                            os.path.join(img_dir,'devkit/cars_meta.mat'),\n",
    "                            cleaned=None,\n",
    "                            transform=transform_test\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size: 8144\n",
      "test size: 8041\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64,\n",
    "                            shuffle=True, num_workers=1)\n",
    "print(\"train size:\", len(train_dataset))\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=64,\n",
    "                            shuffle=True, num_workers=1)\n",
    "print(\"test size:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
