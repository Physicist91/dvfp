{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Deep Vision\" architecture\n",
    "\n",
    "![title](../img/arch-simplified.png)\n",
    "\n",
    "<img src=\"../img/side-branch.png\" alt=\"drawing\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# begin by importing our \"Deep Vision\" module (or dv in short)\n",
    "import dv\n",
    "from dv.model import DeepVision_VGG16\n",
    "from dv.ImageFolder import CarsDataset\n",
    "import dv.helpers as helpers\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = False\n",
    "num_workers = 4\n",
    "batch_size = 64\n",
    "best_top1 = 0\n",
    "resume = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paper use input size of 448 x 448, we will use random crop to this size\n",
    "\n",
    "def transform_train():\n",
    "    transform_list = []\n",
    "    transform_list.append(torchvision.transforms.Lambda(lambda x:helpers.rescale(x, 448)))\n",
    "    transform_list.append(torchvision.transforms.RandomHorizontalFlip(p=0.3))\n",
    "    transform_list.append(torchvision.transforms.RandomCrop((448, 448)))\n",
    "    transform_list.append(torchvision.transforms.ToTensor())\n",
    "    transform_list.append(torchvision.transforms.Normalize(mean=(0.5,0.5,0.5),std=(0.5,0.5,0.5)))\n",
    "    return torchvision.transforms.Compose(transform_list)\n",
    "\n",
    "def transform_test():\n",
    "    transform_list = []\n",
    "    transform_list.append(torchvision.transforms.Lambda(lambda x:helpers.rescale(x, 560)))\n",
    "    transform_list.append(torchvision.transforms.TenCrop(448)) \n",
    "    transform_list.append(torchvision.transforms.Lambda(lambda crops: torch.stack([torchvision.transforms.Normalize(mean=(0.5,0.5,0.5),std=(0.5,0.5,0.5))((torchvision.transforms.ToTensor())(crop)) for crop in crops])) )\n",
    "    return torchvision.transforms.Compose(transform_list)\n",
    "\n",
    "def transform_test_noTC():\n",
    "    transform_list = []\n",
    "    transform_list.append(torchvision.transforms.Lambda(lambda x:helpers.rescale(x, 448)))\n",
    "    transform_list.append(torchvision.transforms.CenterCrop((448, 448)))\n",
    "    transform_list.append(torchvision.transforms.ToTensor())\n",
    "    transform_list.append(torchvision.transforms.Normalize(mean=(0.5,0.5,0.5),std=(0.5,0.5,0.5)))\n",
    "    return torchvision.transforms.Compose(transform_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr0 = 0.1 #initial learning rate\n",
    "momentum = 0.9\n",
    "weight_decay = 0.000005\n",
    "net = DeepVision_VGG16(k = 10, M = 200)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=lr0, momentum=momentum, weight_decay = weight_decay)\n",
    "\n",
    "# Optionally resume from a checkpoint\n",
    "if resume:\n",
    "        if os.path.isfile(args.resume):\n",
    "            checkpoint = torch.load(args.resume)\n",
    "            args.start_epoch = checkpoint['epoch']\n",
    "            best_prec1 = checkpoint['best_prec1']\n",
    "            model.load_state_dict(checkpoint['state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            print('DFL-CNN <==> Part2 : Load Network  <==> Continue from {} epoch {}'.format(args.resume, checkpoint['epoch']))\n",
    "        else:\n",
    "            print('DFL-CNN <==> Part2 : Load Network  <==> Failed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset is from https://ai.stanford.edu/~jkrause/cars/car_dataset.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size: 8144\n",
      "test size: 8041\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "\n",
    "img_dir = '/Users/kevinsiswandi/dvfp/data'\n",
    "train_dataset = CarsDataset(os.path.join(img_dir,'devkit/cars_train_annos.mat'),\n",
    "                            os.path.join(img_dir,'cars_train'),\n",
    "                            os.path.join(img_dir,'devkit/cars_meta.mat'),\n",
    "                            transform=transform_train()\n",
    "                            )\n",
    "\n",
    "test_dataset = CarsDataset(os.path.join(img_dir,'devkit/cars_test_annos_withlabels.mat'),\n",
    "                            os.path.join(img_dir,'cars_test'),\n",
    "                            os.path.join(img_dir,'devkit/cars_meta.mat'),\n",
    "                            transform=transform_test()\n",
    "                            )\n",
    "\n",
    "test_dataset_noTC = CarsDataset(os.path.join(img_dir,'devkit/cars_test_annos_withlabels.mat'),\n",
    "                            os.path.join(img_dir,'cars_test'),\n",
    "                            os.path.join(img_dir,'devkit/cars_meta.mat'),\n",
    "                            transform=transform_test_noTC()\n",
    "                            )\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size,\n",
    "                            shuffle=True, num_workers=num_workers)\n",
    "print(\"train size:\", len(train_dataset))\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size,\n",
    "                            shuffle=True, num_workers=num_workers)\n",
    "\n",
    "test_loader_noTC = DataLoader(test_dataset_noTC, batch_size=batch_size,\n",
    "                            shuffle=True, num_workers=num_workers)\n",
    "\n",
    "print(\"test size:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch: 0 [0/128] || Loss: 11.1782 ||\n",
      "Loss G-Stream: 5.3060 || Loss P-Stream: 5.3425 || Loss side branch: 5.2964 ||\n",
      "Top-1 Acc: 1.56 || Top-5 Acc: 1.56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0xb2392a518>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 675, in __del__\n",
      "    def __del__(self):\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-3801886143fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mout_g\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mloss_g\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_g\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dvfp/dv/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;31m# get feature map (output of conv4_3)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mfeature_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv4_3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;31m# G-stream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    336\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    337\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 338\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training\n",
    "\n",
    "for epoch in range(1):\n",
    "    lr = lr0 * 0.9 ** (epoch//10) #decay the learning rate every 10 epochs\n",
    "    for group in optimizer.param_groups:\n",
    "        group['lr'] = lr\n",
    "    net.train()\n",
    "    \n",
    "    running_loss = 0\n",
    "    running_loss_g = 0\n",
    "    running_loss_p = 0\n",
    "    running_loss_s = 0\n",
    "    correct_top1 = 0\n",
    "    correct_top5 = 0\n",
    "    checkpoints = 20 #for printing loss\n",
    "    \n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "        labels = labels.type(torch.LongTensor)\n",
    "        \n",
    "        # forward pass\n",
    "        out_g, out_p, out_s, _ = net(inputs)\n",
    "\n",
    "        loss_g = criterion(out_g, labels)\n",
    "        loss_p = criterion(out_p, labels)\n",
    "        loss_s = criterion(out_s, labels)\n",
    "        \n",
    "        loss = 1.0 * loss_g + 1.0 * loss_p + 0.1 * loss_s # the weights for each stream as in the paper\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        assert not np.isnan(running_loss), \"Fuck loss blows up!\"\n",
    "        \n",
    "        running_loss_g += loss_g.item()\n",
    "        running_loss_p += loss_p.item()\n",
    "        running_loss_s += loss_s.item()\n",
    "        \n",
    "        out = 1.0 * out_g + 1.0 * out_p + 0.1 * out_s \n",
    "        top1, top5 = dv.helpers.get_accuracy(out, labels, topk=(1, 5))  # paper cited only top-1\n",
    "        \n",
    "        correct_top1 += top1.item()\n",
    "        correct_top5 += top5.item()\n",
    "        \n",
    "        # backward pass + optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % checkpoints == 0:\n",
    "            losses = np.array([running_loss, running_loss_g, running_loss_p, running_loss_s, correct_top1, correct_top5])/(i+1)\n",
    "            print('Train epoch: {} [{}/{}] || Loss: {:.4f} ||\\n'\n",
    "            'Loss G-Stream: {:.4f} || Loss P-Stream: {:.4f} || Loss side branch: {:.4f} ||\\n'\n",
    "            'Top-1 Acc: {:.2f} || Top-5 Acc: {:.2f}'.format(\n",
    "            epoch, i, len(train_loader), losses[0], losses[1], losses[2], losses[3], losses[4], losses[5]))\n",
    "            \n",
    "            helpers.save_train_info(epoch, i, len(train_loader), losses)\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to show an image\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    #plt.show()\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(test_loader_noTC)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate on test set\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for i, data in enumerate(test_loader):\n",
    "    \n",
    "    images, labels = data\n",
    "    \n",
    "    if use_gpu:\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "        \n",
    "    for j, image in enumerate(images): # shape [batchsize, 10_crop, 3, 448, 448]\n",
    "        image = image.unsqueeze(0) # image [1, 3, 448, 448]\n",
    "        out_g, out_p, out_s, _ = net(image)\n",
    "        out = out_g + out_p + 0.1 * out_s\n",
    "        \n",
    "        top1, top5 = dv.helpers.get_accuracy(out, labels, topk=(1, 5))  # paper cited only top-1\n",
    "        correct += top1\n",
    "        total += 1\n",
    "        print('Test <==> Img:{} Top1 {:.3f} Top5 {:.3f}'.format(i, top1.cpu().numpy()[0], top5.cpu().numpy()[0]))\n",
    "\n",
    "print('Test Total <==> Top1 {:.3f}%'.format(correct/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember best prec@1 and save checkpoint\n",
    "\n",
    "save_checkpoint({\n",
    "                'epoch': epoch + 1,\n",
    "                'state_dict': net.state_dict(),\n",
    "                'best_top1': best_top1,\n",
    "                'optimizer' : optimizer.state_dict(),\n",
    "                'top1'     : correct/total,\n",
    "            }, is_best=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
