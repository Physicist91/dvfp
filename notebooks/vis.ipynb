{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data loading and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Vision (+:===:+) PART 1 : setup (+:===:+) (*^o^*) Begin\n",
      "Deep Vision (+:===:+) PART 1 : setup (+:===:+) *\\(^o^)/* End\n",
      "Deep Vision (+:===:+) PART 2 : loading dataset (+:===:+) (*^o^*) Begin\n",
      "train size: 8144\n",
      "test size without tencrop: 8041\n",
      "test size: 8041\n",
      "Deep Vision (+:===:+) PART 2 : loading dataset (+:===:+) *\\(^o^)/* End\n"
     ]
    }
   ],
   "source": [
    "# begin by importing our \"Deep Vision\" module (or dv in short)\n",
    "import dv\n",
    "from dv.model import DeepVision_VGG16\n",
    "from dv.ImageFolder import CarsDataset\n",
    "import dv.helpers as helpers\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "settings = {'use_gpu':False,\n",
    "            'num_epochs':2,\n",
    "    'num_workers':4,\n",
    "    'batch_size':4,\n",
    "    'lr0':0.1, #initial learning rate\n",
    "    'lr_updates':10, # stepsize frequency of learning rate decay\n",
    "    'lr_gamma':0.1, #how much to drop the learning rate\n",
    "    'momentum':0.9,\n",
    "    'weight_decay':0.000005,\n",
    "    'resume':False, #path to model checkpoint\n",
    "    'data_dir':'/Users/kevinsiswandi/dvfp/data'\n",
    "}\n",
    "\n",
    "# Paper use input size of 448 x 448, we will use random crop to this size\n",
    "def transform_train():\n",
    "    transform_list = []\n",
    "    transform_list.append(transforms.Lambda(lambda x:helpers.rescale(x, 448)))\n",
    "    transform_list.append(transforms.RandomHorizontalFlip(p=0.3))\n",
    "    transform_list.append(transforms.RandomCrop((448, 448)))\n",
    "    transform_list.append(transforms.ToTensor())\n",
    "    transform_list.append(transforms.Normalize(mean=(0.5,0.5,0.5),std=(0.5,0.5,0.5)))\n",
    "    return transforms.Compose(transform_list)\n",
    "\n",
    "def transform_test():\n",
    "    transform_list = []\n",
    "    transform_list.append(transforms.Lambda(lambda x:helpers.rescale(x, 560)))\n",
    "    transform_list.append(transforms.TenCrop(448))\n",
    "    transform_list.append(transforms.Lambda(lambda crops: torch.stack([transforms.Normalize(mean=(0.5,0.5,0.5),std=(0.5,0.5,0.5))((transforms.ToTensor())(crop)) for crop in crops])) )\n",
    "    return transforms.Compose(transform_list)\n",
    "\n",
    "def transform_test_noTC():\n",
    "    transform_list = []\n",
    "    transform_list.append(transforms.Lambda(lambda x:helpers.rescale(x, 448)))\n",
    "    transform_list.append(transforms.CenterCrop((448, 448)))\n",
    "    transform_list.append(transforms.ToTensor())\n",
    "    transform_list.append(transforms.Normalize(mean=(0.5,0.5,0.5),std=(0.5,0.5,0.5)))\n",
    "    return transforms.Compose(transform_list)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    print('Deep Vision (+:===:+) PART 1 : setup (+:===:+) (*^o^*) Begin')\n",
    "\n",
    "    lr0 = settings['lr0']\n",
    "    lr_updates = settings['lr_updates']\n",
    "    lr_gamma = settings['lr_gamma']\n",
    "    mom = settings['momentum']\n",
    "    wdecay = settings['weight_decay']\n",
    "    m = settings['batch_size']\n",
    "    img_dir = settings['data_dir']\n",
    "    nworkers = settings['num_workers']\n",
    "    num_epochs = settings['num_epochs']\n",
    "\n",
    "    net = DeepVision_VGG16(k = 10, M = 200)\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if settings['use_gpu']:\n",
    "        print(\"Using \", torch.cuda.device_count(), \" GPUs...\")\n",
    "        net = torch.nn.DataParallel(net)\n",
    "        net = net.to(device)\n",
    "        #cudnn.benchmark = True\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=lr0, momentum=mom, weight_decay = wdecay)\n",
    "    lr_policy = lr_scheduler.StepLR(optimizer, step_size=lr_updates, gamma=lr_gamma)\n",
    "\n",
    "    # Optionally resume from a checkpoint\n",
    "    if settings['resume']:\n",
    "\n",
    "        if os.path.isfile(settings['resume']):\n",
    "            # load stuffs\n",
    "            checkpoint = torch.load(settings['resume'])\n",
    "            start_epoch = checkpoint['epoch']\n",
    "            best_top1 = checkpoint['best_top1']\n",
    "            net.load_state_dict(checkpoint['state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            print('Loading Network  <==> Continue from {} at epoch #{}'.format(settings['resume'], checkpoint['epoch']))\n",
    "\n",
    "        else:\n",
    "            print('Loading Network  <==> Failed')\n",
    "    else:\n",
    "        best_top1 = 0\n",
    "\n",
    "    print('Deep Vision (+:===:+) PART 1 : setup (+:===:+) *\\(^o^)/* End')\n",
    "\n",
    "    ####################################################################\n",
    "    ####################################################################\n",
    "    print('Deep Vision (+:===:+) PART 2 : loading dataset (+:===:+) (*^o^*) Begin')\n",
    "\n",
    "    train_dataset = CarsDataset(os.path.join(img_dir,'devkit/cars_train_annos.mat'),\n",
    "                            os.path.join(img_dir,'cars_train'),\n",
    "                            os.path.join(img_dir,'devkit/cars_meta.mat'),\n",
    "                            transform=transform_train()\n",
    "                            )\n",
    "\n",
    "    test_dataset = CarsDataset(os.path.join(img_dir,'devkit/cars_test_annos_withlabels.mat'),\n",
    "                            os.path.join(img_dir,'cars_test'),\n",
    "                            os.path.join(img_dir,'devkit/cars_meta.mat'),\n",
    "                            transform=transform_test()\n",
    "                            )\n",
    "\n",
    "    test_dataset_noTC = CarsDataset(os.path.join(img_dir,'devkit/cars_test_annos_withlabels.mat'),\n",
    "                            os.path.join(img_dir,'cars_test'),\n",
    "                            os.path.join(img_dir,'devkit/cars_meta.mat'),\n",
    "                            transform=transform_test_noTC()\n",
    "                            )\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=m, shuffle=True, num_workers=nworkers, pin_memory=True)\n",
    "    print(\"train size:\", len(train_dataset))\n",
    "    test_loader = DataLoader(test_dataset, batch_size=m, shuffle=True, num_workers=nworkers, pin_memory=True)\n",
    "    test_loader_noTC = DataLoader(test_dataset_noTC, batch_size=m, shuffle=True, num_workers=nworkers, pin_memory=True)\n",
    "    print(\"test size without tencrop:\", len(test_dataset_noTC))\n",
    "    print(\"test size:\", len(test_dataset))\n",
    "    \n",
    "\n",
    "    print('Deep Vision (+:===:+) PART 2 : loading dataset (+:===:+) *\\(^o^)/* End')\n",
    "    #######################################################\n",
    "    #######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# may need to get classes\n",
    "#car_classes = train_dataset.map_class()\n",
    "\n",
    "# load model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predict image label and draw boxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = './vis_result'\n",
    "if not os.path.isdir(result_dir):\n",
    "        os.mkdir(result_dir)\n",
    "\n",
    "img_dir = './vis_image'\n",
    "num_imgs = len(os.listdir(img_dir))\n",
    "\n",
    "for img in range(num_imgs):\n",
    "    img_path = os.path.join(img_dir, '{}.jpg'.format(img))\n",
    "    \n",
    "    # for prediction\n",
    "    transform1 = transform_test_noTC() \n",
    "    # for visualization\n",
    "    transform2 = transforms.Compose([\n",
    "                                transforms.Resize(448),\n",
    "                                transforms.CenterCrop((448, 448)),\n",
    "                                transforms.Pad((42, 42))\n",
    "                            ])\n",
    "    \n",
    "    img = Image.open(img_path)\n",
    "    \n",
    "    img_pad = transform2(img)\n",
    "    img_tensor = transform1(img)\n",
    "    img_tensor = img_tensor.unsqueeze(0)\n",
    "    out1, out2, out3, indices = model(img_tensor)\n",
    "    out = out1 + out2 + 0.1 *out3\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
