{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is re-do of the tutorial from https://www.analyticsvidhya.com/blog/2018/02/pytorch-tutorial/\n",
    "\n",
    "**Remark**: the tutorial contained some mistakes in the PyTorch implementation, which I corrected here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8245, 0.6841, 0.0948],\n",
      "        [0.4273, 0.9448, 0.7295],\n",
      "        [0.5441, 0.7802, 0.9139]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.8245, 0.4273, 0.5441],\n",
       "        [0.6841, 0.9448, 0.7802],\n",
       "        [0.0948, 0.7295, 0.9139]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix = torch.rand(3, 3)\n",
    "print(matrix)\n",
    "matrix.t()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network in Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual :\n",
      " [[1]\n",
      " [1]\n",
      " [0]] \n",
      "\n",
      "predicted :\n",
      " [[0.97879314]\n",
      " [0.97184513]\n",
      " [0.03776725]]\n"
     ]
    }
   ],
   "source": [
    "# Input array\n",
    "X = np.array([[1,0,1,0],[1,0,1,1],[0,1,0,1]])\n",
    "\n",
    "# Output array\n",
    "y = np.array([[1],[1],[0]])\n",
    "\n",
    "# Sigmoid Function\n",
    "def sigmoid (x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "# Derivative of Sigmoid\n",
    "def dSigmoid(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Variable initialization\n",
    "epoch = 5000 # training iterations\n",
    "lr = 0.1 # learning rate\n",
    "nInput = X.shape[1] # input layer size: number of features in data set\n",
    "nHidden = 3 # number of hidden layers neurons\n",
    "nOutput = 1 # output layer size\n",
    "\n",
    "# Weight and bias initialization\n",
    "w_h = np.random.uniform(size=(nInput, nHidden))\n",
    "b_h = np.random.uniform(size=(1, nHidden))\n",
    "w_out = np.random.uniform(size=(nHidden, nOutput))\n",
    "b_out = np.random.uniform(size=(1, nOutput))\n",
    "\n",
    "# training\n",
    "for i in range(epoch):\n",
    "  \n",
    "    # Forward Propagation\n",
    "    a_h = np.dot(X, w_h) + b_h\n",
    "    a_h = sigmoid(a_h)\n",
    "    a_out = np.dot(a_h, w_out) + b_out\n",
    "    a_out = sigmoid(a_out)\n",
    "\n",
    "    # Backpropagation\n",
    "    dy = y - a_out\n",
    "    dy = dy * dSigmoid(a_out)\n",
    "    da = dy.dot(w_out.T)\n",
    "    da = da * dSigmoid(a_h)\n",
    "    w_out += a_h.T.dot(dy) * lr\n",
    "    b_out += np.sum(dy, axis=0, keepdims=True) * lr\n",
    "    w_h += X.T.dot(da) * lr\n",
    "    b_h += np.sum(da, axis=0, keepdims=True) * lr\n",
    "\n",
    "print('actual :\\n', y, '\\n')\n",
    "print('predicted :\\n', a_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual :\n",
      " tensor([[1.],\n",
      "        [1.],\n",
      "        [0.]]) \n",
      "\n",
      "predicted :\n",
      " tensor([[0.9796],\n",
      "        [0.9705],\n",
      "        [0.0469]])\n"
     ]
    }
   ],
   "source": [
    "#Input array\n",
    "X = torch.Tensor([[1,0,1,0],[1,0,1,1],[0,1,0,1]])\n",
    "\n",
    "#Output\n",
    "y = torch.Tensor([[1],[1],[0]])\n",
    "\n",
    "#Sigmoid Function\n",
    "def sigmoid (x):\n",
    "    return 1/(1 + torch.exp(-x))\n",
    "\n",
    "#Derivative of Sigmoid Function\n",
    "def dSigmoid(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Variable initialization\n",
    "epoch = 5000 # training iterations\n",
    "lr = 0.1 # learning rate\n",
    "nInput = X.shape[1] # input layer size: number of features in data set\n",
    "nHidden = 3 # number of hidden layers neurons\n",
    "nOutput = 1 # output layer size\n",
    "\n",
    "#weight and bias initialization\n",
    "w_h = torch.randn(nInput, nHidden).type(torch.FloatTensor)\n",
    "b_h = torch.randn(1, nHidden).type(torch.FloatTensor)\n",
    "w_out = torch.randn(nHidden, nOutput)\n",
    "b_out = torch.randn(1, nOutput)\n",
    "\n",
    "for i in range(epoch):\n",
    "\n",
    "    #Forward propagation\n",
    "    a_h = torch.mm(X, w_h) + b_h\n",
    "    a_h = sigmoid(a_h)\n",
    "    a_out = torch.mm(a_h, w_out) + b_out\n",
    "    a_out = sigmoid(a_out)\n",
    "\n",
    "    #Backpropagation\n",
    "    dy = y - a_out\n",
    "    dy = dy * dSigmoid(a_out)\n",
    "    da = torch.mm(dy, w_out.t())\n",
    "    da = da * dSigmoid(a_h)\n",
    "    w_out += torch.mm(a_h.t(), dy) * lr\n",
    "    b_out += dy.sum() * lr\n",
    "    w_h += torch.mm(X.t(), da) * lr\n",
    "    b_h += da.sum() * lr\n",
    " \n",
    "print('actual :\\n', y, '\\n')\n",
    "print('predicted :\\n', a_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
